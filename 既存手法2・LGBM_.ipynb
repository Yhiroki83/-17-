{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import warnings\n",
        "import gc\n",
        "import time\n",
        "from IPython.core.display import Image\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_rows', 100)"
      ],
      "metadata": {
        "id": "BE3nyqnPMVS7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall --yes lightgbm && pip install --install-option=--gpu lightgbm"
      ],
      "metadata": {
        "id": "biK9BVnbZ-Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for_train2 = pd.read_csv(\"./data/for_train2.csv\")\n",
        "for_test2 = pd.read_csv(\"./data/for_test2.csv\")\n",
        "df_missing = pd.read_csv(\"./data/df_missing.csv\")\n",
        "#unique_id = for_train[\"id\"].astype(int).unique()"
      ],
      "metadata": {
        "id": "5cdtAQhRech3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "folds = 5\n",
        "\n",
        "test3 = pd.DataFrame()\n",
        "scores3 = pd.DataFrame()\n",
        "for id in df_missing[df_missing.missing_percent_test!=1].id.unique():\n",
        "  id = int(id)\n",
        "  for_train2_id = for_train2[for_train2.id==id]\n",
        "  for_test2_id = for_test2[for_test2.id==id]\n",
        "\n",
        "  scores2 = pd.DataFrame(columns=[\"id\",\"month\",\"train.shape[0]\",\"train_mape\",\"valid_mape\",\"test_mape\",\"train_rmse\",\"valid_rmse\",\"test_rmse\"])\n",
        "  month_list = [6,5,4,3,2,1,12,11,10,9,8]\n",
        "  train2 = for_train2_id[for_train2_id.month.isin(month_list)].reset_index(drop=True)\n",
        "  test2 = for_test2_id[for_test2_id.month==7].reset_index(drop=True)\n",
        "\n",
        "  features1 = [\"year\",\"month\",\"day\",\"time\",\"minute\",\"id\",\"pvrate\",\"id_lat\",\"id_lng\"] + [f\"preal-30_{id}\"]\n",
        "  target = [\"nv2\"]\n",
        "  params = {\n",
        "      'boosting_type': 'gbdt',\n",
        "      'metric': 'rmse',\n",
        "      'objective': 'regression',\n",
        "      'device':'gpu'\n",
        "  }\n",
        "  te_preds = []\n",
        "  kf = KFold(n_splits=folds, shuffle=True,random_state=81)\n",
        "  for fold,(tr_idx, va_idx) in enumerate(kf.split(train2)):\n",
        "    lgb_results = []\n",
        "\n",
        "    train_set = lgb.Dataset(train2.loc[train2.index.isin(tr_idx),features1],train2.loc[train2.index.isin(tr_idx),target])\n",
        "    val_set = lgb.Dataset(train2.loc[train2.index.isin(va_idx),features1],train2.loc[train2.index.isin(va_idx),target])\n",
        "    # Train and evaluate\n",
        "    lgb_result = {} \n",
        "    model = lgb.train(\n",
        "        params, \n",
        "        train_set, \n",
        "        num_boost_round = 100000, \n",
        "        early_stopping_rounds = 100, \n",
        "        valid_sets = [train_set, val_set], \n",
        "        verbose_eval = 100,\n",
        "        evals_result = lgb_result,\n",
        "    )\n",
        "    lgb_results.append(lgb_result)\n",
        "\n",
        "    model_path = f\"./output/single/single_lgbm_{id}_{fold}.pkl\"\n",
        "    joblib.dump(model, model_path)\n",
        "    tt = joblib.load(model_path)\n",
        "\n",
        "    train2.loc[train2.index.isin(va_idx),\"pred\"] = tt.predict(train2.loc[train2.index.isin(va_idx),features1])\n",
        "    te_preds.append(tt.predict(test2[features1]))\n",
        "\n",
        "    \n",
        "  test2[\"pred\"] = np.mean(np.array(te_preds),axis=0)\n",
        "  scores2= scores2.append({\"id\":id,\n",
        "                \"month\":month_list,\"train.shape[0]\":train2.shape[0],\n",
        "                \"train_mape\":(np.abs((train2.loc[train2.nv2!=0,\"pred\"] - train2.loc[train2.nv2!=0,\"nv2\"])/train2.loc[train2.nv2!=0,\"nv2\"])).mean(),\n",
        "                #\"valid_mape\":(np.abs((valid2.loc[valid2.nv2!=0,\"pred\"] - valid2.loc[valid2.nv2!=0,\"nv2\"])/valid2.loc[valid2.nv2!=0,\"nv2\"])).mean(),\n",
        "                \"test_mape\":(np.abs((test2.loc[test2.nv2!=0,\"pred\"] - test2.loc[test2.nv2!=0,\"nv2\"])/test2.loc[test2.nv2!=0,\"nv2\"])).mean(),\n",
        "                \"train_rmse\":np.sqrt(mean_squared_error(train2[\"nv2\"], train2[\"pred\"])),\n",
        "                #\"valid_rmse\":np.sqrt(mean_squared_error(valid2[\"nv2\"], valid2[\"pred\"])),\n",
        "                \"test_rmse\":np.sqrt(mean_squared_error(test2[\"nv2\"], test2[\"pred\"])),\n",
        "                \"train_mae\":np.abs(train2[\"nv2\"]-train2[\"pred\"]).mean(),\n",
        "                #\"valid_mae\":np.abs(valid2[\"nv2\"]-valid2[\"pred\"]).mean(),\n",
        "                \"test_mae\":np.abs(test2[\"nv2\"]-test2[\"pred\"]).mean(),\n",
        "                }\n",
        "              ,ignore_index=True)\n",
        "  \n",
        "  scores3 = pd.concat([scores3,scores2],axis=0)\n",
        "  test3 = pd.concat([test3,test2],axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFJJ6NRYAMvK",
        "outputId": "ee2f1271-ff44-4733-bd7c-112410f0e6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n",
            "[100]\ttraining's rmse: 0.0646037\tvalid_1's rmse: 0.0859403\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0709153\tvalid_1's rmse: 0.0857377\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000476 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.371594\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.066527\tvalid_1's rmse: 0.0817637\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.0713251\tvalid_1's rmse: 0.0810734\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000481 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.372470\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0667445\tvalid_1's rmse: 0.0776973\n",
            "Early stopping, best iteration is:\n",
            "[62]\ttraining's rmse: 0.0700427\tvalid_1's rmse: 0.0774293\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000495 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.373815\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0668883\tvalid_1's rmse: 0.0748335\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.070918\tvalid_1's rmse: 0.0742292\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375196\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0683677\tvalid_1's rmse: 0.0907572\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttraining's rmse: 0.0711192\tvalid_1's rmse: 0.0900891\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000477 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.371761\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0679358\tvalid_1's rmse: 0.089468\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0750138\tvalid_1's rmse: 0.0890281\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.003060 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374276\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0700628\tvalid_1's rmse: 0.0840863\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0730576\tvalid_1's rmse: 0.0837545\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000469 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374535\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.069686\tvalid_1's rmse: 0.0856848\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's rmse: 0.0772579\tvalid_1's rmse: 0.0842839\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000480 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.376379\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0708603\tvalid_1's rmse: 0.0778535\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.0742909\tvalid_1's rmse: 0.0770156\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000514 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.336971\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0628376\tvalid_1's rmse: 0.0791795\n",
            "[200]\ttraining's rmse: 0.0567191\tvalid_1's rmse: 0.0797738\n",
            "Early stopping, best iteration is:\n",
            "[129]\ttraining's rmse: 0.0606415\tvalid_1's rmse: 0.0790688\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.333906\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0621922\tvalid_1's rmse: 0.0793246\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0670374\tvalid_1's rmse: 0.0788229\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.335323\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0630733\tvalid_1's rmse: 0.0774943\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0688886\tvalid_1's rmse: 0.0769452\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000457 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.337301\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0631232\tvalid_1's rmse: 0.0762936\n",
            "Early stopping, best iteration is:\n",
            "[87]\ttraining's rmse: 0.0641016\tvalid_1's rmse: 0.0761168\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000463 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.337887\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0640541\tvalid_1's rmse: 0.07348\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0701494\tvalid_1's rmse: 0.0731295\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000486 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.327219\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0701283\tvalid_1's rmse: 0.0903196\n",
            "Early stopping, best iteration is:\n",
            "[78]\ttraining's rmse: 0.0719996\tvalid_1's rmse: 0.0900091\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.323418\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0693291\tvalid_1's rmse: 0.0913685\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0747843\tvalid_1's rmse: 0.091139\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000483 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.325120\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0708628\tvalid_1's rmse: 0.0847118\n",
            "Early stopping, best iteration is:\n",
            "[98]\ttraining's rmse: 0.0709903\tvalid_1's rmse: 0.0846816\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000463 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.326062\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0702701\tvalid_1's rmse: 0.0882942\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's rmse: 0.0773135\tvalid_1's rmse: 0.0872206\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000474 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.327859\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0718787\tvalid_1's rmse: 0.0802166\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0776455\tvalid_1's rmse: 0.0799297\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000481 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.379734\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0628489\tvalid_1's rmse: 0.0795025\n",
            "Early stopping, best iteration is:\n",
            "[54]\ttraining's rmse: 0.0670436\tvalid_1's rmse: 0.0792626\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000449 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.378143\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0630486\tvalid_1's rmse: 0.0814602\n",
            "Early stopping, best iteration is:\n",
            "[82]\ttraining's rmse: 0.0643614\tvalid_1's rmse: 0.0814092\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000525 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.378819\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0641016\tvalid_1's rmse: 0.0763375\n",
            "Early stopping, best iteration is:\n",
            "[78]\ttraining's rmse: 0.0655622\tvalid_1's rmse: 0.0762152\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380293\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0642876\tvalid_1's rmse: 0.0747309\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0684788\tvalid_1's rmse: 0.0744898\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000486 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380800\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0639169\tvalid_1's rmse: 0.074568\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0691639\tvalid_1's rmse: 0.0739366\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000471 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381152\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0624007\tvalid_1's rmse: 0.0795411\n",
            "Early stopping, best iteration is:\n",
            "[52]\ttraining's rmse: 0.0671162\tvalid_1's rmse: 0.0789956\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.379350\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0624595\tvalid_1's rmse: 0.0806812\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's rmse: 0.0647997\tvalid_1's rmse: 0.0806062\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000461 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380046\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0642331\tvalid_1's rmse: 0.0755753\n",
            "Early stopping, best iteration is:\n",
            "[78]\ttraining's rmse: 0.0658782\tvalid_1's rmse: 0.0755597\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000461 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381403\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.064105\tvalid_1's rmse: 0.0750332\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's rmse: 0.0665185\tvalid_1's rmse: 0.0748382\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000774 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.382079\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0638906\tvalid_1's rmse: 0.0750526\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0689157\tvalid_1's rmse: 0.0743088\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000491 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.371151\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0627298\tvalid_1's rmse: 0.0788731\n",
            "Early stopping, best iteration is:\n",
            "[62]\ttraining's rmse: 0.0658567\tvalid_1's rmse: 0.078794\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000504 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.369670\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0627045\tvalid_1's rmse: 0.0812023\n",
            "[200]\ttraining's rmse: 0.0564102\tvalid_1's rmse: 0.0820092\n",
            "Early stopping, best iteration is:\n",
            "[102]\ttraining's rmse: 0.0625773\tvalid_1's rmse: 0.0811589\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000516 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.370348\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0643049\tvalid_1's rmse: 0.0763645\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0677144\tvalid_1's rmse: 0.0759529\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000702 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.371799\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0637221\tvalid_1's rmse: 0.0750122\n",
            "Early stopping, best iteration is:\n",
            "[86]\ttraining's rmse: 0.0649131\tvalid_1's rmse: 0.0748322\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000486 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.372454\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0637163\tvalid_1's rmse: 0.0744344\n",
            "Early stopping, best iteration is:\n",
            "[79]\ttraining's rmse: 0.0655417\tvalid_1's rmse: 0.0741006\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380934\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.062452\tvalid_1's rmse: 0.0789237\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0672314\tvalid_1's rmse: 0.078518\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.379198\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0623745\tvalid_1's rmse: 0.0797294\n",
            "Early stopping, best iteration is:\n",
            "[74]\ttraining's rmse: 0.064411\tvalid_1's rmse: 0.079418\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000462 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.379839\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0634518\tvalid_1's rmse: 0.0754944\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0668926\tvalid_1's rmse: 0.0750268\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000500 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381179\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0631259\tvalid_1's rmse: 0.0745226\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.0658405\tvalid_1's rmse: 0.0743151\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000515 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381828\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0633744\tvalid_1's rmse: 0.0741259\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0683962\tvalid_1's rmse: 0.0735469\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381969\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0629319\tvalid_1's rmse: 0.0801949\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's rmse: 0.0693339\tvalid_1's rmse: 0.079564\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.379277\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0633674\tvalid_1's rmse: 0.0790174\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0688702\tvalid_1's rmse: 0.0787041\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000502 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380259\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0635196\tvalid_1's rmse: 0.0804722\n",
            "Early stopping, best iteration is:\n",
            "[45]\ttraining's rmse: 0.0684936\tvalid_1's rmse: 0.0799212\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000474 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381280\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0650942\tvalid_1's rmse: 0.0740021\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0684601\tvalid_1's rmse: 0.073718\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.382234\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0644074\tvalid_1's rmse: 0.0750314\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.0672187\tvalid_1's rmse: 0.0746569\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000472 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366084\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0641186\tvalid_1's rmse: 0.0838502\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's rmse: 0.0686659\tvalid_1's rmse: 0.0831243\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000509 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363592\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0646653\tvalid_1's rmse: 0.0796137\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttraining's rmse: 0.0671396\tvalid_1's rmse: 0.079273\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.364513\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0643166\tvalid_1's rmse: 0.0811281\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.0674063\tvalid_1's rmse: 0.0806055\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365669\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0653834\tvalid_1's rmse: 0.0765266\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0703863\tvalid_1's rmse: 0.0758534\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366876\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0659236\tvalid_1's rmse: 0.0754986\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0689559\tvalid_1's rmse: 0.0751477\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000722 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.351165\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0620411\tvalid_1's rmse: 0.0821454\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0660373\tvalid_1's rmse: 0.0817415\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000599 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349155\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0628619\tvalid_1's rmse: 0.0797013\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0667978\tvalid_1's rmse: 0.0787406\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000471 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.350369\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0635881\tvalid_1's rmse: 0.0764624\n",
            "Early stopping, best iteration is:\n",
            "[53]\ttraining's rmse: 0.0682245\tvalid_1's rmse: 0.0761273\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000483 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.350798\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0632687\tvalid_1's rmse: 0.0775271\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0687129\tvalid_1's rmse: 0.0768928\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000490 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.352079\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0624025\tvalid_1's rmse: 0.0794257\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0693257\tvalid_1's rmse: 0.0781839\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000495 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.357361\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0648805\tvalid_1's rmse: 0.0807301\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0690395\tvalid_1's rmse: 0.0804997\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000474 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.354949\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0655161\tvalid_1's rmse: 0.0797424\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0693816\tvalid_1's rmse: 0.0793779\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000483 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.356045\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0644199\tvalid_1's rmse: 0.0832523\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's rmse: 0.070658\tvalid_1's rmse: 0.0825508\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000451 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.357285\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0650457\tvalid_1's rmse: 0.079205\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's rmse: 0.0720223\tvalid_1's rmse: 0.0779545\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000505 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.357859\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0654528\tvalid_1's rmse: 0.0779497\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0702094\tvalid_1's rmse: 0.0773849\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000750 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.348809\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0651481\tvalid_1's rmse: 0.0836455\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0695493\tvalid_1's rmse: 0.0832798\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000462 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.345554\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.064703\tvalid_1's rmse: 0.0843177\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0688333\tvalid_1's rmse: 0.0835745\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000506 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.348262\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0669984\tvalid_1's rmse: 0.0775793\n",
            "[200]\ttraining's rmse: 0.0605304\tvalid_1's rmse: 0.077436\n",
            "Early stopping, best iteration is:\n",
            "[175]\ttraining's rmse: 0.0620068\tvalid_1's rmse: 0.0771222\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000451 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.348201\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0655623\tvalid_1's rmse: 0.0807644\n",
            "Early stopping, best iteration is:\n",
            "[55]\ttraining's rmse: 0.0699322\tvalid_1's rmse: 0.0803625\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000497 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349115\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0667706\tvalid_1's rmse: 0.0785188\n",
            "Early stopping, best iteration is:\n",
            "[84]\ttraining's rmse: 0.0681027\tvalid_1's rmse: 0.0783593\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.354322\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0700381\tvalid_1's rmse: 0.0902605\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0762066\tvalid_1's rmse: 0.0898759\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.351453\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0707635\tvalid_1's rmse: 0.0860902\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's rmse: 0.0778333\tvalid_1's rmse: 0.0850886\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000483 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.353149\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.070921\tvalid_1's rmse: 0.086669\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0750009\tvalid_1's rmse: 0.0861665\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.354543\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0705989\tvalid_1's rmse: 0.0891978\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's rmse: 0.0779255\tvalid_1's rmse: 0.088478\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.355802\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0714405\tvalid_1's rmse: 0.0828204\n",
            "Early stopping, best iteration is:\n",
            "[52]\ttraining's rmse: 0.0762429\tvalid_1's rmse: 0.0824017\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.350770\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0639632\tvalid_1's rmse: 0.0768118\n",
            "Early stopping, best iteration is:\n",
            "[75]\ttraining's rmse: 0.0661703\tvalid_1's rmse: 0.0764794\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.003614 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.347463\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0627172\tvalid_1's rmse: 0.0838588\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's rmse: 0.0691651\tvalid_1's rmse: 0.0828021\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000461 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349645\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0638486\tvalid_1's rmse: 0.078404\n",
            "Early stopping, best iteration is:\n",
            "[80]\ttraining's rmse: 0.06537\tvalid_1's rmse: 0.0782637\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000481 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.350212\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0628381\tvalid_1's rmse: 0.0828466\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0668252\tvalid_1's rmse: 0.0820807\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000480 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.351145\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.064625\tvalid_1's rmse: 0.0755517\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0709199\tvalid_1's rmse: 0.0751755\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000484 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.318468\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0673189\tvalid_1's rmse: 0.081383\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0711213\tvalid_1's rmse: 0.0812081\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000494 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.315527\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0654598\tvalid_1's rmse: 0.0883239\n",
            "Early stopping, best iteration is:\n",
            "[70]\ttraining's rmse: 0.0683067\tvalid_1's rmse: 0.0881127\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.318136\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0681395\tvalid_1's rmse: 0.0815645\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.071779\tvalid_1's rmse: 0.0808795\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000497 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.319183\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.068001\tvalid_1's rmse: 0.0799692\n",
            "Early stopping, best iteration is:\n",
            "[86]\ttraining's rmse: 0.0694049\tvalid_1's rmse: 0.0798682\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.319895\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0673827\tvalid_1's rmse: 0.0810654\n",
            "Early stopping, best iteration is:\n",
            "[73]\ttraining's rmse: 0.0699328\tvalid_1's rmse: 0.0805216\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000480 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.354193\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0646552\tvalid_1's rmse: 0.079803\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0712575\tvalid_1's rmse: 0.0784516\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000457 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.350966\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0633734\tvalid_1's rmse: 0.0871431\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0680639\tvalid_1's rmse: 0.0864803\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.351658\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0649741\tvalid_1's rmse: 0.0807646\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0690468\tvalid_1's rmse: 0.0801015\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.354358\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.065552\tvalid_1's rmse: 0.0797819\n",
            "Early stopping, best iteration is:\n",
            "[66]\ttraining's rmse: 0.0686202\tvalid_1's rmse: 0.079645\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.354266\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.065036\tvalid_1's rmse: 0.0805513\n",
            "Early stopping, best iteration is:\n",
            "[80]\ttraining's rmse: 0.0668817\tvalid_1's rmse: 0.0802391\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.358286\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.067319\tvalid_1's rmse: 0.0882018\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.0712415\tvalid_1's rmse: 0.0878264\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000506 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.353710\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0684009\tvalid_1's rmse: 0.0834649\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0725514\tvalid_1's rmse: 0.0825443\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000471 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.355776\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0680239\tvalid_1's rmse: 0.0880621\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0718432\tvalid_1's rmse: 0.0877664\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000970 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.356980\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0688373\tvalid_1's rmse: 0.0811117\n",
            "Early stopping, best iteration is:\n",
            "[88]\ttraining's rmse: 0.070092\tvalid_1's rmse: 0.0808329\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000528 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.358070\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0685362\tvalid_1's rmse: 0.0778165\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0732393\tvalid_1's rmse: 0.0772732\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000462 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361541\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0663796\tvalid_1's rmse: 0.0827181\n",
            "Early stopping, best iteration is:\n",
            "[68]\ttraining's rmse: 0.0691871\tvalid_1's rmse: 0.0824279\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.358025\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0652195\tvalid_1's rmse: 0.0875063\n",
            "Early stopping, best iteration is:\n",
            "[62]\ttraining's rmse: 0.0684598\tvalid_1's rmse: 0.0871619\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000480 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.360385\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0671388\tvalid_1's rmse: 0.0806696\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0721771\tvalid_1's rmse: 0.0798112\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000548 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361605\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0668491\tvalid_1's rmse: 0.0813268\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttraining's rmse: 0.0695217\tvalid_1's rmse: 0.0811505\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.362113\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.067732\tvalid_1's rmse: 0.0783711\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.073361\tvalid_1's rmse: 0.0779613\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000483 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384520\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0700416\tvalid_1's rmse: 0.088385\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0764881\tvalid_1's rmse: 0.0879144\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000490 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380665\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0691009\tvalid_1's rmse: 0.0922012\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's rmse: 0.0741552\tvalid_1's rmse: 0.0917877\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000740 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383315\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.070528\tvalid_1's rmse: 0.0853498\n",
            "Early stopping, best iteration is:\n",
            "[60]\ttraining's rmse: 0.074631\tvalid_1's rmse: 0.0848929\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384273\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0704836\tvalid_1's rmse: 0.0855649\n",
            "Early stopping, best iteration is:\n",
            "[89]\ttraining's rmse: 0.0713156\tvalid_1's rmse: 0.085465\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000617 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.385197\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0717618\tvalid_1's rmse: 0.0810123\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0753788\tvalid_1's rmse: 0.080496\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.001568 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.333193\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0648995\tvalid_1's rmse: 0.0796169\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's rmse: 0.0708533\tvalid_1's rmse: 0.0791639\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.330341\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0641874\tvalid_1's rmse: 0.0836835\n",
            "[200]\ttraining's rmse: 0.057375\tvalid_1's rmse: 0.083661\n",
            "Early stopping, best iteration is:\n",
            "[181]\ttraining's rmse: 0.0582548\tvalid_1's rmse: 0.0834098\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000502 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.333221\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0655056\tvalid_1's rmse: 0.0799075\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0689438\tvalid_1's rmse: 0.0794306\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000495 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.333255\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0643691\tvalid_1's rmse: 0.0821764\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.0696288\tvalid_1's rmse: 0.0820917\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000484 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.334319\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0665751\tvalid_1's rmse: 0.0767307\n",
            "[200]\ttraining's rmse: 0.0597288\tvalid_1's rmse: 0.0774547\n",
            "Early stopping, best iteration is:\n",
            "[106]\ttraining's rmse: 0.0660991\tvalid_1's rmse: 0.0766657\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000630 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.338649\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0669909\tvalid_1's rmse: 0.0811945\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.0713626\tvalid_1's rmse: 0.081132\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000502 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.335367\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0678229\tvalid_1's rmse: 0.0796502\n",
            "[200]\ttraining's rmse: 0.0611178\tvalid_1's rmse: 0.0801178\n",
            "Early stopping, best iteration is:\n",
            "[112]\ttraining's rmse: 0.0669111\tvalid_1's rmse: 0.0795308\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000486 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.335955\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0672173\tvalid_1's rmse: 0.083391\n",
            "[200]\ttraining's rmse: 0.0606373\tvalid_1's rmse: 0.0843091\n",
            "Early stopping, best iteration is:\n",
            "[108]\ttraining's rmse: 0.0666711\tvalid_1's rmse: 0.083314\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000489 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.338634\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0672594\tvalid_1's rmse: 0.0818326\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0714779\tvalid_1's rmse: 0.0815908\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000484 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.338461\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0668184\tvalid_1's rmse: 0.0827451\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0711529\tvalid_1's rmse: 0.082374\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000531 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375383\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0676819\tvalid_1's rmse: 0.0870326\n",
            "Early stopping, best iteration is:\n",
            "[52]\ttraining's rmse: 0.0730578\tvalid_1's rmse: 0.086255\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.370716\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0674003\tvalid_1's rmse: 0.0877225\n",
            "Early stopping, best iteration is:\n",
            "[54]\ttraining's rmse: 0.0725079\tvalid_1's rmse: 0.0867629\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000476 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.373069\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0686258\tvalid_1's rmse: 0.0855072\n",
            "Early stopping, best iteration is:\n",
            "[88]\ttraining's rmse: 0.0697456\tvalid_1's rmse: 0.0853405\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374211\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0684981\tvalid_1's rmse: 0.0849111\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0727766\tvalid_1's rmse: 0.0844631\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000506 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375969\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0699831\tvalid_1's rmse: 0.0786664\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0757591\tvalid_1's rmse: 0.0782273\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000475 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383927\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0650724\tvalid_1's rmse: 0.077008\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.070698\tvalid_1's rmse: 0.0765202\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000489 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381078\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0643204\tvalid_1's rmse: 0.0835044\n",
            "Early stopping, best iteration is:\n",
            "[53]\ttraining's rmse: 0.0685085\tvalid_1's rmse: 0.0831195\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381970\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0658494\tvalid_1's rmse: 0.0791981\n",
            "[200]\ttraining's rmse: 0.0601071\tvalid_1's rmse: 0.0797211\n",
            "Early stopping, best iteration is:\n",
            "[113]\ttraining's rmse: 0.0650553\tvalid_1's rmse: 0.0789718\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.001082 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384037\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0649314\tvalid_1's rmse: 0.0810321\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0684081\tvalid_1's rmse: 0.0807438\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.385176\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0662235\tvalid_1's rmse: 0.0755258\n",
            "Early stopping, best iteration is:\n",
            "[82]\ttraining's rmse: 0.0675383\tvalid_1's rmse: 0.0754271\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000807 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.357433\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0638491\tvalid_1's rmse: 0.0796285\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0668163\tvalid_1's rmse: 0.0791588\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.354978\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0637687\tvalid_1's rmse: 0.0780223\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's rmse: 0.0697794\tvalid_1's rmse: 0.0772675\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000488 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.356442\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0640195\tvalid_1's rmse: 0.0785687\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0697733\tvalid_1's rmse: 0.0775748\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.358558\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0636514\tvalid_1's rmse: 0.078384\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0698273\tvalid_1's rmse: 0.077877\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.358383\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0644057\tvalid_1's rmse: 0.0759019\n",
            "Early stopping, best iteration is:\n",
            "[54]\ttraining's rmse: 0.0685755\tvalid_1's rmse: 0.0754517\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000476 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.367494\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0644418\tvalid_1's rmse: 0.0804645\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0676812\tvalid_1's rmse: 0.0799398\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365452\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.064459\tvalid_1's rmse: 0.0788693\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0706475\tvalid_1's rmse: 0.0780859\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366626\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0645188\tvalid_1's rmse: 0.0793197\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0680414\tvalid_1's rmse: 0.0788478\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000474 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.368304\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0644613\tvalid_1's rmse: 0.079256\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0703053\tvalid_1's rmse: 0.0788307\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000486 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.368567\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.065296\tvalid_1's rmse: 0.0762082\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.0686662\tvalid_1's rmse: 0.0759226\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000517 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.367174\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0668186\tvalid_1's rmse: 0.0838754\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0707701\tvalid_1's rmse: 0.0836274\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000469 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.364445\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0669083\tvalid_1's rmse: 0.0816169\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0733002\tvalid_1's rmse: 0.0808279\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000501 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366161\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0671237\tvalid_1's rmse: 0.0829906\n",
            "Early stopping, best iteration is:\n",
            "[55]\ttraining's rmse: 0.0711491\tvalid_1's rmse: 0.0823777\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.367622\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0673481\tvalid_1's rmse: 0.082764\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.072091\tvalid_1's rmse: 0.0823625\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.368299\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0675461\tvalid_1's rmse: 0.0801804\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0717711\tvalid_1's rmse: 0.0792325\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375841\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.061119\tvalid_1's rmse: 0.0787794\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0656478\tvalid_1's rmse: 0.0785063\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000484 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.372790\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0612606\tvalid_1's rmse: 0.0779584\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's rmse: 0.0676505\tvalid_1's rmse: 0.0774907\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000486 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374513\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.062134\tvalid_1's rmse: 0.0744541\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0675547\tvalid_1's rmse: 0.0736818\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375955\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0620057\tvalid_1's rmse: 0.0754315\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.065455\tvalid_1's rmse: 0.0748601\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000455 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.377082\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0629631\tvalid_1's rmse: 0.0704172\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0668773\tvalid_1's rmse: 0.0699829\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000490 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361320\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0624065\tvalid_1's rmse: 0.0815308\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0684782\tvalid_1's rmse: 0.0808111\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000443 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.358265\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0626811\tvalid_1's rmse: 0.0804338\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0666009\tvalid_1's rmse: 0.0801995\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000463 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.359399\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0641453\tvalid_1's rmse: 0.0758423\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0690558\tvalid_1's rmse: 0.0754944\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361517\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.06316\tvalid_1's rmse: 0.0802015\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0683594\tvalid_1's rmse: 0.0797338\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000451 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361571\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0644312\tvalid_1's rmse: 0.0758432\n",
            "Early stopping, best iteration is:\n",
            "[70]\ttraining's rmse: 0.0669338\tvalid_1's rmse: 0.0751712\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.282716\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0593994\tvalid_1's rmse: 0.0804949\n",
            "[200]\ttraining's rmse: 0.0533755\tvalid_1's rmse: 0.0807014\n",
            "Early stopping, best iteration is:\n",
            "[110]\ttraining's rmse: 0.0586514\tvalid_1's rmse: 0.0803302\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000478 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.278910\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0599507\tvalid_1's rmse: 0.0790102\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's rmse: 0.0650414\tvalid_1's rmse: 0.0789753\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000490 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.281138\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0605639\tvalid_1's rmse: 0.0762992\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0635458\tvalid_1's rmse: 0.0756603\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000461 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.281287\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0612659\tvalid_1's rmse: 0.0724153\n",
            "Early stopping, best iteration is:\n",
            "[75]\ttraining's rmse: 0.0634479\tvalid_1's rmse: 0.0721435\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000484 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.281323\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0617377\tvalid_1's rmse: 0.0683077\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0669075\tvalid_1's rmse: 0.0676453\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000500 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380547\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.076156\tvalid_1's rmse: 0.0962072\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's rmse: 0.0827522\tvalid_1's rmse: 0.0952924\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.376229\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0745251\tvalid_1's rmse: 0.102985\n",
            "Early stopping, best iteration is:\n",
            "[35]\ttraining's rmse: 0.0838219\tvalid_1's rmse: 0.102126\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.377981\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0762024\tvalid_1's rmse: 0.0952331\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0836644\tvalid_1's rmse: 0.0947153\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 316\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.378826\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0765974\tvalid_1's rmse: 0.0938548\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0832612\tvalid_1's rmse: 0.0930202\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380699\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0783395\tvalid_1's rmse: 0.0853419\n",
            "Early stopping, best iteration is:\n",
            "[95]\ttraining's rmse: 0.0788372\tvalid_1's rmse: 0.0853002\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.419366\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0689867\tvalid_1's rmse: 0.0896156\n",
            "Early stopping, best iteration is:\n",
            "[45]\ttraining's rmse: 0.0749476\tvalid_1's rmse: 0.0886984\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000499 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.415837\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0689218\tvalid_1's rmse: 0.0889522\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's rmse: 0.0765162\tvalid_1's rmse: 0.0874317\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000476 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.417963\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0700029\tvalid_1's rmse: 0.0858537\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0740186\tvalid_1's rmse: 0.0854392\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.418788\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0698899\tvalid_1's rmse: 0.0848718\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.073754\tvalid_1's rmse: 0.0844618\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000462 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.420544\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0706305\tvalid_1's rmse: 0.0791737\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0747981\tvalid_1's rmse: 0.0788894\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000498 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.386131\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0714355\tvalid_1's rmse: 0.0919281\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0779728\tvalid_1's rmse: 0.0913037\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000473 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381909\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0720897\tvalid_1's rmse: 0.0885996\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0784956\tvalid_1's rmse: 0.0877271\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000476 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383305\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0703058\tvalid_1's rmse: 0.0926104\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's rmse: 0.077911\tvalid_1's rmse: 0.0919409\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000477 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384711\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0715848\tvalid_1's rmse: 0.0896033\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.077216\tvalid_1's rmse: 0.0894863\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000474 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.385834\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0733808\tvalid_1's rmse: 0.0802103\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0780877\tvalid_1's rmse: 0.0793469\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349885\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0661495\tvalid_1's rmse: 0.0847801\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0721141\tvalid_1's rmse: 0.0845971\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000462 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.345966\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0666653\tvalid_1's rmse: 0.0800339\n",
            "Early stopping, best iteration is:\n",
            "[84]\ttraining's rmse: 0.0680401\tvalid_1's rmse: 0.0799321\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000469 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.347411\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0653437\tvalid_1's rmse: 0.0854824\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0690148\tvalid_1's rmse: 0.0849902\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.003366 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.348553\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0664913\tvalid_1's rmse: 0.0830733\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0721878\tvalid_1's rmse: 0.082681\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349457\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0671429\tvalid_1's rmse: 0.0752594\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0728958\tvalid_1's rmse: 0.0747101\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000468 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365657\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.066663\tvalid_1's rmse: 0.0874922\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.0717138\tvalid_1's rmse: 0.0872392\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000463 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361882\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.068379\tvalid_1's rmse: 0.0816251\n",
            "Early stopping, best iteration is:\n",
            "[83]\ttraining's rmse: 0.0697701\tvalid_1's rmse: 0.0815118\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000496 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363175\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0668035\tvalid_1's rmse: 0.0882818\n",
            "Early stopping, best iteration is:\n",
            "[62]\ttraining's rmse: 0.0703363\tvalid_1's rmse: 0.087665\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000478 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.364420\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.067855\tvalid_1's rmse: 0.085031\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0740167\tvalid_1's rmse: 0.0848886\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000504 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365396\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0688049\tvalid_1's rmse: 0.0771348\n",
            "Early stopping, best iteration is:\n",
            "[55]\ttraining's rmse: 0.073559\tvalid_1's rmse: 0.0767188\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000537 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365850\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0668705\tvalid_1's rmse: 0.0862694\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's rmse: 0.0724301\tvalid_1's rmse: 0.0851134\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000492 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.362821\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0674882\tvalid_1's rmse: 0.0854917\n",
            "Early stopping, best iteration is:\n",
            "[79]\ttraining's rmse: 0.0688908\tvalid_1's rmse: 0.0854318\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000504 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363635\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0680687\tvalid_1's rmse: 0.0836691\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0709712\tvalid_1's rmse: 0.0834208\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000489 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365581\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0679545\tvalid_1's rmse: 0.0842909\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's rmse: 0.0749709\tvalid_1's rmse: 0.0833213\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0689851\tvalid_1's rmse: 0.0773502\n",
            "Early stopping, best iteration is:\n",
            "[73]\ttraining's rmse: 0.0714519\tvalid_1's rmse: 0.0770107\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000469 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.299809\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0582008\tvalid_1's rmse: 0.0766775\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.062004\tvalid_1's rmse: 0.0760819\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000557 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.298844\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0585739\tvalid_1's rmse: 0.0785808\n",
            "Early stopping, best iteration is:\n",
            "[76]\ttraining's rmse: 0.060481\tvalid_1's rmse: 0.0780506\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000463 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.299615\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0598049\tvalid_1's rmse: 0.0714259\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0644806\tvalid_1's rmse: 0.07078\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000477 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.301050\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0596948\tvalid_1's rmse: 0.0730163\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0627794\tvalid_1's rmse: 0.0725072\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000510 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.300458\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0601422\tvalid_1's rmse: 0.0701324\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0641137\tvalid_1's rmse: 0.069645\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000451 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.351254\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0555193\tvalid_1's rmse: 0.0696183\n",
            "Early stopping, best iteration is:\n",
            "[35]\ttraining's rmse: 0.0619931\tvalid_1's rmse: 0.0694191\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000472 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.348669\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0550889\tvalid_1's rmse: 0.0693672\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0594479\tvalid_1's rmse: 0.0688357\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000454 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349395\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0554374\tvalid_1's rmse: 0.0692865\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0596049\tvalid_1's rmse: 0.0688356\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000476 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.351509\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0559744\tvalid_1's rmse: 0.0673767\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0605575\tvalid_1's rmse: 0.0670305\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.002030 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.350852\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0568089\tvalid_1's rmse: 0.0645773\n",
            "Early stopping, best iteration is:\n",
            "[62]\ttraining's rmse: 0.0596418\tvalid_1's rmse: 0.0641799\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.362197\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0626034\tvalid_1's rmse: 0.0763157\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0682092\tvalid_1's rmse: 0.0760748\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000492 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.358961\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.062507\tvalid_1's rmse: 0.0787845\n",
            "Early stopping, best iteration is:\n",
            "[75]\ttraining's rmse: 0.0646974\tvalid_1's rmse: 0.0781601\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361225\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0634184\tvalid_1's rmse: 0.0766841\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0691285\tvalid_1's rmse: 0.0762554\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.362260\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0623521\tvalid_1's rmse: 0.0811832\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttraining's rmse: 0.0631856\tvalid_1's rmse: 0.0810876\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000490 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363484\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0637076\tvalid_1's rmse: 0.0736008\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.0664311\tvalid_1's rmse: 0.0730933\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000451 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.360701\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0702321\tvalid_1's rmse: 0.0876532\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.0754666\tvalid_1's rmse: 0.0872411\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000490 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.355952\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0686879\tvalid_1's rmse: 0.0929878\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.073033\tvalid_1's rmse: 0.0927595\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000477 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.357795\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.070685\tvalid_1's rmse: 0.0860365\n",
            "Early stopping, best iteration is:\n",
            "[91]\ttraining's rmse: 0.0713874\tvalid_1's rmse: 0.0859708\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.359104\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0693535\tvalid_1's rmse: 0.0889338\n",
            "Early stopping, best iteration is:\n",
            "[45]\ttraining's rmse: 0.0757783\tvalid_1's rmse: 0.0883476\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361514\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0709065\tvalid_1's rmse: 0.0797783\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.0745517\tvalid_1's rmse: 0.0792456\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374778\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0683936\tvalid_1's rmse: 0.0905976\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0747575\tvalid_1's rmse: 0.0891858\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.372502\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0686105\tvalid_1's rmse: 0.0916522\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's rmse: 0.0752276\tvalid_1's rmse: 0.0906679\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000516 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.373515\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0707168\tvalid_1's rmse: 0.0824145\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0748461\tvalid_1's rmse: 0.0818108\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000444 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375121\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0700048\tvalid_1's rmse: 0.0836796\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0763578\tvalid_1's rmse: 0.0834052\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000496 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375388\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0699871\tvalid_1's rmse: 0.0840098\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's rmse: 0.0761365\tvalid_1's rmse: 0.0834384\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000482 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375412\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0643097\tvalid_1's rmse: 0.0841287\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0700984\tvalid_1's rmse: 0.0833845\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000495 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.371078\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0644922\tvalid_1's rmse: 0.0842593\n",
            "Early stopping, best iteration is:\n",
            "[54]\ttraining's rmse: 0.0687983\tvalid_1's rmse: 0.0834988\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000475 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.373631\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.065945\tvalid_1's rmse: 0.0777037\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0723233\tvalid_1's rmse: 0.0773921\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374381\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0654175\tvalid_1's rmse: 0.0836482\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0693659\tvalid_1's rmse: 0.0830042\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375853\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0662504\tvalid_1's rmse: 0.0766827\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's rmse: 0.0687451\tvalid_1's rmse: 0.0762663\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000482 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.396289\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0704185\tvalid_1's rmse: 0.085527\n",
            "Early stopping, best iteration is:\n",
            "[85]\ttraining's rmse: 0.0714728\tvalid_1's rmse: 0.0853511\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000483 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.393792\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0697289\tvalid_1's rmse: 0.0853818\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0732215\tvalid_1's rmse: 0.0850511\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.002525 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.394671\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0688267\tvalid_1's rmse: 0.087816\n",
            "Early stopping, best iteration is:\n",
            "[91]\ttraining's rmse: 0.0695015\tvalid_1's rmse: 0.0877826\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000556 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.397504\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.070063\tvalid_1's rmse: 0.0855718\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttraining's rmse: 0.0707578\tvalid_1's rmse: 0.0855079\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000498 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.397397\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0718002\tvalid_1's rmse: 0.0793415\n",
            "Early stopping, best iteration is:\n",
            "[85]\ttraining's rmse: 0.0731489\tvalid_1's rmse: 0.0792282\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000451 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.351849\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0633168\tvalid_1's rmse: 0.0773115\n",
            "[200]\ttraining's rmse: 0.0576631\tvalid_1's rmse: 0.0775394\n",
            "Early stopping, best iteration is:\n",
            "[137]\ttraining's rmse: 0.0609452\tvalid_1's rmse: 0.0771912\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349403\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0623955\tvalid_1's rmse: 0.0800581\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0663241\tvalid_1's rmse: 0.0797975\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000489 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.350303\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0625653\tvalid_1's rmse: 0.0790116\n",
            "Early stopping, best iteration is:\n",
            "[83]\ttraining's rmse: 0.0636618\tvalid_1's rmse: 0.0788043\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000489 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.353925\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0631655\tvalid_1's rmse: 0.0784382\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's rmse: 0.0652661\tvalid_1's rmse: 0.0782937\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000456 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.352697\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0636132\tvalid_1's rmse: 0.0746763\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttraining's rmse: 0.0643483\tvalid_1's rmse: 0.0746017\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000521 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383319\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.063889\tvalid_1's rmse: 0.0817184\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0690615\tvalid_1's rmse: 0.0814719\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.379609\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0645277\tvalid_1's rmse: 0.0789345\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.0672719\tvalid_1's rmse: 0.0783282\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000896 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380775\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0649442\tvalid_1's rmse: 0.0784478\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttraining's rmse: 0.0677172\tvalid_1's rmse: 0.0781695\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383718\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0634225\tvalid_1's rmse: 0.082464\n",
            "Early stopping, best iteration is:\n",
            "[45]\ttraining's rmse: 0.0694027\tvalid_1's rmse: 0.0818105\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.001809 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383343\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0651876\tvalid_1's rmse: 0.0736639\n",
            "Early stopping, best iteration is:\n",
            "[54]\ttraining's rmse: 0.0699174\tvalid_1's rmse: 0.0728868\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000456 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380626\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0616101\tvalid_1's rmse: 0.0800416\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0678123\tvalid_1's rmse: 0.0792726\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000494 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.376713\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0628045\tvalid_1's rmse: 0.0769798\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's rmse: 0.0684495\tvalid_1's rmse: 0.0765487\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.377987\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0635059\tvalid_1's rmse: 0.076383\n",
            "Early stopping, best iteration is:\n",
            "[63]\ttraining's rmse: 0.0664407\tvalid_1's rmse: 0.0761697\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000463 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380868\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0622391\tvalid_1's rmse: 0.0805951\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0670235\tvalid_1's rmse: 0.0798045\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380441\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.063724\tvalid_1's rmse: 0.0716193\n",
            "Early stopping, best iteration is:\n",
            "[60]\ttraining's rmse: 0.0671404\tvalid_1's rmse: 0.0713121\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000532 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384488\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0627402\tvalid_1's rmse: 0.0802595\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0662502\tvalid_1's rmse: 0.0794658\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000474 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.380497\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0628415\tvalid_1's rmse: 0.077379\n",
            "Early stopping, best iteration is:\n",
            "[68]\ttraining's rmse: 0.0656889\tvalid_1's rmse: 0.0768299\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000507 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381884\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0632596\tvalid_1's rmse: 0.0771425\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0673115\tvalid_1's rmse: 0.0768887\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384668\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0623677\tvalid_1's rmse: 0.0807752\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.067027\tvalid_1's rmse: 0.0797891\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000557 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384341\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0646139\tvalid_1's rmse: 0.0718184\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.067665\tvalid_1's rmse: 0.0714155\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000532 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.369219\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0616091\tvalid_1's rmse: 0.0774275\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's rmse: 0.0671873\tvalid_1's rmse: 0.0761136\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365311\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0626405\tvalid_1's rmse: 0.0726674\n",
            "Early stopping, best iteration is:\n",
            "[52]\ttraining's rmse: 0.0674573\tvalid_1's rmse: 0.072286\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000457 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366437\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0613435\tvalid_1's rmse: 0.0797081\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0665983\tvalid_1's rmse: 0.0789852\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.369257\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0619006\tvalid_1's rmse: 0.0768755\n",
            "Early stopping, best iteration is:\n",
            "[54]\ttraining's rmse: 0.0661054\tvalid_1's rmse: 0.07641\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.369303\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0618591\tvalid_1's rmse: 0.0747837\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0668033\tvalid_1's rmse: 0.0743567\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000494 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.373654\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0655844\tvalid_1's rmse: 0.0816701\n",
            "Early stopping, best iteration is:\n",
            "[78]\ttraining's rmse: 0.0672251\tvalid_1's rmse: 0.0814929\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000495 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.370842\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0660029\tvalid_1's rmse: 0.0794645\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's rmse: 0.0706129\tvalid_1's rmse: 0.0786023\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.372114\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0646161\tvalid_1's rmse: 0.0837146\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's rmse: 0.0708412\tvalid_1's rmse: 0.0830931\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.373892\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0653414\tvalid_1's rmse: 0.0804626\n",
            "Early stopping, best iteration is:\n",
            "[63]\ttraining's rmse: 0.0681723\tvalid_1's rmse: 0.0799187\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000725 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374885\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0658307\tvalid_1's rmse: 0.0750587\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's rmse: 0.0731618\tvalid_1's rmse: 0.0742791\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000448 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.364773\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.068667\tvalid_1's rmse: 0.0860445\n",
            "Early stopping, best iteration is:\n",
            "[63]\ttraining's rmse: 0.071815\tvalid_1's rmse: 0.0858098\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000482 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361579\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0686001\tvalid_1's rmse: 0.0829578\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0742126\tvalid_1's rmse: 0.0821396\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000526 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363019\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0677328\tvalid_1's rmse: 0.0873423\n",
            "Early stopping, best iteration is:\n",
            "[68]\ttraining's rmse: 0.0703707\tvalid_1's rmse: 0.0869099\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.364732\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0687569\tvalid_1's rmse: 0.0840268\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.073543\tvalid_1's rmse: 0.0836322\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000489 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365889\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0689135\tvalid_1's rmse: 0.0787429\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0756863\tvalid_1's rmse: 0.0775368\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000741 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.385292\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0669878\tvalid_1's rmse: 0.0839674\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0703575\tvalid_1's rmse: 0.083584\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381871\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0669475\tvalid_1's rmse: 0.0810407\n",
            "Early stopping, best iteration is:\n",
            "[80]\ttraining's rmse: 0.0687812\tvalid_1's rmse: 0.0808807\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000463 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383633\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0658239\tvalid_1's rmse: 0.0865781\n",
            "Early stopping, best iteration is:\n",
            "[63]\ttraining's rmse: 0.0691119\tvalid_1's rmse: 0.0861742\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.385063\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0667272\tvalid_1's rmse: 0.0817584\n",
            "Early stopping, best iteration is:\n",
            "[72]\ttraining's rmse: 0.0692025\tvalid_1's rmse: 0.0814777\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.386251\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0670834\tvalid_1's rmse: 0.0777486\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's rmse: 0.0747105\tvalid_1's rmse: 0.0760827\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000478 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366116\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0635021\tvalid_1's rmse: 0.0808481\n",
            "[200]\ttraining's rmse: 0.0569774\tvalid_1's rmse: 0.0815281\n",
            "Early stopping, best iteration is:\n",
            "[104]\ttraining's rmse: 0.0631967\tvalid_1's rmse: 0.0808403\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000486 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363812\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0630115\tvalid_1's rmse: 0.0822634\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0698404\tvalid_1's rmse: 0.0810883\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365356\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0638082\tvalid_1's rmse: 0.0774654\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0692833\tvalid_1's rmse: 0.0767773\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000510 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.367177\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0641531\tvalid_1's rmse: 0.0791169\n",
            "Early stopping, best iteration is:\n",
            "[78]\ttraining's rmse: 0.0659212\tvalid_1's rmse: 0.0790012\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366848\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.064659\tvalid_1's rmse: 0.075408\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0684476\tvalid_1's rmse: 0.0752708\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383203\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0672192\tvalid_1's rmse: 0.0842423\n",
            "Early stopping, best iteration is:\n",
            "[70]\ttraining's rmse: 0.0701109\tvalid_1's rmse: 0.0838567\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000593 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.379968\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0661673\tvalid_1's rmse: 0.0864407\n",
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's rmse: 0.0741851\tvalid_1's rmse: 0.0858046\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000515 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.382325\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0681822\tvalid_1's rmse: 0.0808442\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0714714\tvalid_1's rmse: 0.0805872\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000454 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383541\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0670761\tvalid_1's rmse: 0.0841595\n",
            "Early stopping, best iteration is:\n",
            "[60]\ttraining's rmse: 0.0708151\tvalid_1's rmse: 0.0839319\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000486 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384144\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0686308\tvalid_1's rmse: 0.0784476\n",
            "Early stopping, best iteration is:\n",
            "[53]\ttraining's rmse: 0.0732629\tvalid_1's rmse: 0.0781182\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000473 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.322496\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0578925\tvalid_1's rmse: 0.0742186\n",
            "Early stopping, best iteration is:\n",
            "[55]\ttraining's rmse: 0.0623247\tvalid_1's rmse: 0.0732096\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.320915\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0581265\tvalid_1's rmse: 0.0778947\n",
            "Early stopping, best iteration is:\n",
            "[74]\ttraining's rmse: 0.0602671\tvalid_1's rmse: 0.0776011\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000492 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.321774\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0583082\tvalid_1's rmse: 0.0751194\n",
            "Early stopping, best iteration is:\n",
            "[74]\ttraining's rmse: 0.0605644\tvalid_1's rmse: 0.0750095\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000482 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.323165\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0594855\tvalid_1's rmse: 0.0706875\n",
            "[200]\ttraining's rmse: 0.0538641\tvalid_1's rmse: 0.0709352\n",
            "Early stopping, best iteration is:\n",
            "[134]\ttraining's rmse: 0.0571662\tvalid_1's rmse: 0.0704446\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000461 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.324816\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0600227\tvalid_1's rmse: 0.0678478\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0654043\tvalid_1's rmse: 0.0675407\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000558 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.312940\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0567954\tvalid_1's rmse: 0.0696817\n",
            "Early stopping, best iteration is:\n",
            "[55]\ttraining's rmse: 0.0611097\tvalid_1's rmse: 0.0693264\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000476 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.309066\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0550101\tvalid_1's rmse: 0.0771081\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0595049\tvalid_1's rmse: 0.0764697\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000448 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.310905\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.057039\tvalid_1's rmse: 0.0701381\n",
            "Early stopping, best iteration is:\n",
            "[88]\ttraining's rmse: 0.0580412\tvalid_1's rmse: 0.0700312\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000480 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.311485\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0567913\tvalid_1's rmse: 0.070493\n",
            "Early stopping, best iteration is:\n",
            "[80]\ttraining's rmse: 0.0583679\tvalid_1's rmse: 0.0703547\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000536 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.311954\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0573212\tvalid_1's rmse: 0.0664464\n",
            "Early stopping, best iteration is:\n",
            "[68]\ttraining's rmse: 0.0603\tvalid_1's rmse: 0.0661798\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000479 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.392964\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0688425\tvalid_1's rmse: 0.0886171\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's rmse: 0.0768204\tvalid_1's rmse: 0.0878541\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000541 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.389186\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0686903\tvalid_1's rmse: 0.0878805\n",
            "Early stopping, best iteration is:\n",
            "[55]\ttraining's rmse: 0.0731801\tvalid_1's rmse: 0.0874755\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000731 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.391447\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0688356\tvalid_1's rmse: 0.0854725\n",
            "Early stopping, best iteration is:\n",
            "[95]\ttraining's rmse: 0.069399\tvalid_1's rmse: 0.0852794\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000471 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.392957\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0698954\tvalid_1's rmse: 0.0833381\n",
            "Early stopping, best iteration is:\n",
            "[78]\ttraining's rmse: 0.0718344\tvalid_1's rmse: 0.0832963\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000498 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.393906\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0708337\tvalid_1's rmse: 0.0799926\n",
            "Early stopping, best iteration is:\n",
            "[68]\ttraining's rmse: 0.0737791\tvalid_1's rmse: 0.0795992\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000462 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375217\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0669833\tvalid_1's rmse: 0.0848383\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.0698674\tvalid_1's rmse: 0.0844076\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000468 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.371442\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0665175\tvalid_1's rmse: 0.0843619\n",
            "Early stopping, best iteration is:\n",
            "[65]\ttraining's rmse: 0.0699492\tvalid_1's rmse: 0.0839738\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.373193\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0672034\tvalid_1's rmse: 0.0822323\n",
            "Early stopping, best iteration is:\n",
            "[65]\ttraining's rmse: 0.0703537\tvalid_1's rmse: 0.0817035\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000484 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374385\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0666566\tvalid_1's rmse: 0.0840574\n",
            "Early stopping, best iteration is:\n",
            "[75]\ttraining's rmse: 0.068849\tvalid_1's rmse: 0.0840175\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000450 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375814\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0680823\tvalid_1's rmse: 0.0768663\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0747914\tvalid_1's rmse: 0.0762845\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000520 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.397504\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0640574\tvalid_1's rmse: 0.0833991\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0706685\tvalid_1's rmse: 0.0827504\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000476 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.394125\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0659236\tvalid_1's rmse: 0.077368\n",
            "Early stopping, best iteration is:\n",
            "[83]\ttraining's rmse: 0.0675399\tvalid_1's rmse: 0.0772074\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.395668\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0650877\tvalid_1's rmse: 0.0841275\n",
            "Early stopping, best iteration is:\n",
            "[74]\ttraining's rmse: 0.0670502\tvalid_1's rmse: 0.0836484\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000469 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.397294\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0662458\tvalid_1's rmse: 0.0765705\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0706225\tvalid_1's rmse: 0.0764414\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.398593\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.06547\tvalid_1's rmse: 0.0797397\n",
            "Early stopping, best iteration is:\n",
            "[55]\ttraining's rmse: 0.0696286\tvalid_1's rmse: 0.0793182\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000523 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.347859\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0567397\tvalid_1's rmse: 0.0714433\n",
            "Early stopping, best iteration is:\n",
            "[82]\ttraining's rmse: 0.0581006\tvalid_1's rmse: 0.0712469\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000469 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.346460\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0560302\tvalid_1's rmse: 0.0741429\n",
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's rmse: 0.0632234\tvalid_1's rmse: 0.0731235\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.346668\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0572302\tvalid_1's rmse: 0.0709337\n",
            "Early stopping, best iteration is:\n",
            "[80]\ttraining's rmse: 0.0584831\tvalid_1's rmse: 0.0708758\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349419\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0576109\tvalid_1's rmse: 0.0684612\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0613375\tvalid_1's rmse: 0.067655\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000492 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.349394\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0580523\tvalid_1's rmse: 0.0673904\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's rmse: 0.0602386\tvalid_1's rmse: 0.0669574\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.418953\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0660611\tvalid_1's rmse: 0.0843651\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0706202\tvalid_1's rmse: 0.0837035\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.416894\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0647705\tvalid_1's rmse: 0.0877708\n",
            "Early stopping, best iteration is:\n",
            "[63]\ttraining's rmse: 0.0683849\tvalid_1's rmse: 0.0871041\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.417680\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0665695\tvalid_1's rmse: 0.0818018\n",
            "Early stopping, best iteration is:\n",
            "[55]\ttraining's rmse: 0.0711604\tvalid_1's rmse: 0.0814494\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000757 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.420320\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0664924\tvalid_1's rmse: 0.080349\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0705927\tvalid_1's rmse: 0.079563\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.420387\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0678822\tvalid_1's rmse: 0.0756467\n",
            "Early stopping, best iteration is:\n",
            "[88]\ttraining's rmse: 0.0688518\tvalid_1's rmse: 0.0755059\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.324917\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0581124\tvalid_1's rmse: 0.0727549\n",
            "Early stopping, best iteration is:\n",
            "[85]\ttraining's rmse: 0.0594057\tvalid_1's rmse: 0.0725531\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000489 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.324343\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0573268\tvalid_1's rmse: 0.0762753\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's rmse: 0.0634687\tvalid_1's rmse: 0.0757586\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.324216\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0581286\tvalid_1's rmse: 0.071705\n",
            "Early stopping, best iteration is:\n",
            "[78]\ttraining's rmse: 0.0598783\tvalid_1's rmse: 0.0715006\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000455 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.326803\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0588852\tvalid_1's rmse: 0.0698554\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.0635152\tvalid_1's rmse: 0.0689405\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.009789 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.327199\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0596262\tvalid_1's rmse: 0.066975\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0625114\tvalid_1's rmse: 0.0668597\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000975 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.316224\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0567156\tvalid_1's rmse: 0.0683807\n",
            "Early stopping, best iteration is:\n",
            "[73]\ttraining's rmse: 0.059015\tvalid_1's rmse: 0.0679886\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000472 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.313313\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0558986\tvalid_1's rmse: 0.0718609\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0612063\tvalid_1's rmse: 0.0714603\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.314954\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0565033\tvalid_1's rmse: 0.0699481\n",
            "Early stopping, best iteration is:\n",
            "[62]\ttraining's rmse: 0.0592366\tvalid_1's rmse: 0.0697187\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.316560\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0557734\tvalid_1's rmse: 0.0728382\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's rmse: 0.0625342\tvalid_1's rmse: 0.0724027\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000510 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.315373\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0580675\tvalid_1's rmse: 0.0658149\n",
            "Early stopping, best iteration is:\n",
            "[95]\ttraining's rmse: 0.0583804\tvalid_1's rmse: 0.0656754\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.329365\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0595934\tvalid_1's rmse: 0.0773037\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.0637657\tvalid_1's rmse: 0.0767583\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000473 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.327330\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0595257\tvalid_1's rmse: 0.0762594\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0640064\tvalid_1's rmse: 0.0757686\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.328834\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0601324\tvalid_1's rmse: 0.0737956\n",
            "[200]\ttraining's rmse: 0.0545039\tvalid_1's rmse: 0.074116\n",
            "Early stopping, best iteration is:\n",
            "[132]\ttraining's rmse: 0.0580666\tvalid_1's rmse: 0.0737353\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000445 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.330119\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0607665\tvalid_1's rmse: 0.0726035\n",
            "Early stopping, best iteration is:\n",
            "[68]\ttraining's rmse: 0.063164\tvalid_1's rmse: 0.0724289\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000494 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.331209\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0608225\tvalid_1's rmse: 0.0688636\n",
            "Early stopping, best iteration is:\n",
            "[79]\ttraining's rmse: 0.0624299\tvalid_1's rmse: 0.0686952\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000704 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.275575\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0540823\tvalid_1's rmse: 0.06951\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's rmse: 0.0609018\tvalid_1's rmse: 0.0683866\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000446 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.271341\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0536338\tvalid_1's rmse: 0.0717923\n",
            "Early stopping, best iteration is:\n",
            "[83]\ttraining's rmse: 0.0547608\tvalid_1's rmse: 0.0715933\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.273783\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0548557\tvalid_1's rmse: 0.0665064\n",
            "Early stopping, best iteration is:\n",
            "[86]\ttraining's rmse: 0.0558017\tvalid_1's rmse: 0.0662549\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.274184\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.053824\tvalid_1's rmse: 0.070429\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's rmse: 0.0598896\tvalid_1's rmse: 0.0695288\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.274108\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0549748\tvalid_1's rmse: 0.0636667\n",
            "Early stopping, best iteration is:\n",
            "[62]\ttraining's rmse: 0.0582415\tvalid_1's rmse: 0.0633514\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.377404\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0615632\tvalid_1's rmse: 0.0782776\n",
            "Early stopping, best iteration is:\n",
            "[94]\ttraining's rmse: 0.061949\tvalid_1's rmse: 0.07822\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000502 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375746\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0608181\tvalid_1's rmse: 0.0779831\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's rmse: 0.0669436\tvalid_1's rmse: 0.0772605\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000480 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.376856\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0622928\tvalid_1's rmse: 0.0757884\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0662099\tvalid_1's rmse: 0.0755171\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000492 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.378388\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0619286\tvalid_1's rmse: 0.07571\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's rmse: 0.0644146\tvalid_1's rmse: 0.0754151\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000529 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.378906\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0628993\tvalid_1's rmse: 0.0712349\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0688075\tvalid_1's rmse: 0.0703726\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.370559\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0655863\tvalid_1's rmse: 0.0824019\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's rmse: 0.0723017\tvalid_1's rmse: 0.0818251\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000552 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.366249\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0653049\tvalid_1's rmse: 0.0851382\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.0691649\tvalid_1's rmse: 0.0846801\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000457 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.367686\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.06633\tvalid_1's rmse: 0.0788456\n",
            "Early stopping, best iteration is:\n",
            "[66]\ttraining's rmse: 0.069659\tvalid_1's rmse: 0.0783484\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000461 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.369975\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0661067\tvalid_1's rmse: 0.0851871\n",
            "[200]\ttraining's rmse: 0.059482\tvalid_1's rmse: 0.0852648\n",
            "Early stopping, best iteration is:\n",
            "[142]\ttraining's rmse: 0.062876\tvalid_1's rmse: 0.0850513\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000459 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.370600\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0673093\tvalid_1's rmse: 0.0759243\n",
            "[200]\ttraining's rmse: 0.0600755\tvalid_1's rmse: 0.0765723\n",
            "Early stopping, best iteration is:\n",
            "[108]\ttraining's rmse: 0.0665049\tvalid_1's rmse: 0.0756704\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000454 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.378794\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0652456\tvalid_1's rmse: 0.0811293\n",
            "Early stopping, best iteration is:\n",
            "[75]\ttraining's rmse: 0.0672603\tvalid_1's rmse: 0.0808146\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.374791\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0662075\tvalid_1's rmse: 0.0767571\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.0696863\tvalid_1's rmse: 0.0764689\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000490 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.375923\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0642372\tvalid_1's rmse: 0.0840992\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0699541\tvalid_1's rmse: 0.0834135\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000464 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.378677\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0654433\tvalid_1's rmse: 0.0802875\n",
            "Early stopping, best iteration is:\n",
            "[72]\ttraining's rmse: 0.0674574\tvalid_1's rmse: 0.0802271\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000484 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.379242\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0661013\tvalid_1's rmse: 0.0773846\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0720104\tvalid_1's rmse: 0.0773017\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000456 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363220\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0646132\tvalid_1's rmse: 0.0795719\n",
            "Early stopping, best iteration is:\n",
            "[60]\ttraining's rmse: 0.0684925\tvalid_1's rmse: 0.0791692\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000504 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.360416\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0634312\tvalid_1's rmse: 0.0837907\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0708826\tvalid_1's rmse: 0.0825026\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000500 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361003\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0643934\tvalid_1's rmse: 0.0801566\n",
            "Early stopping, best iteration is:\n",
            "[53]\ttraining's rmse: 0.0688139\tvalid_1's rmse: 0.0796561\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363463\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0652943\tvalid_1's rmse: 0.0773958\n",
            "Early stopping, best iteration is:\n",
            "[54]\ttraining's rmse: 0.0695806\tvalid_1's rmse: 0.077032\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000491 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363730\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0656869\tvalid_1's rmse: 0.0760903\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.0705616\tvalid_1's rmse: 0.0753661\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.391208\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0683661\tvalid_1's rmse: 0.0855023\n",
            "Early stopping, best iteration is:\n",
            "[52]\ttraining's rmse: 0.0732604\tvalid_1's rmse: 0.0851535\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000474 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.387186\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0694142\tvalid_1's rmse: 0.0802142\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0734904\tvalid_1's rmse: 0.0798405\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.388718\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0671253\tvalid_1's rmse: 0.0887039\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0715497\tvalid_1's rmse: 0.0883402\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000466 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.390612\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0691824\tvalid_1's rmse: 0.0808003\n",
            "[200]\ttraining's rmse: 0.0624706\tvalid_1's rmse: 0.0809033\n",
            "Early stopping, best iteration is:\n",
            "[103]\ttraining's rmse: 0.0689569\tvalid_1's rmse: 0.0807755\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.391644\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0692063\tvalid_1's rmse: 0.0822905\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0742885\tvalid_1's rmse: 0.0819973\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.368497\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.065643\tvalid_1's rmse: 0.0869228\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's rmse: 0.0731282\tvalid_1's rmse: 0.0861737\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000473 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.364650\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0662781\tvalid_1's rmse: 0.0843561\n",
            "Early stopping, best iteration is:\n",
            "[53]\ttraining's rmse: 0.0708968\tvalid_1's rmse: 0.0840949\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000468 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.365857\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0662632\tvalid_1's rmse: 0.084949\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0712129\tvalid_1's rmse: 0.0844979\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.368028\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0676329\tvalid_1's rmse: 0.0774295\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's rmse: 0.0733111\tvalid_1's rmse: 0.0770444\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.368672\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0676851\tvalid_1's rmse: 0.0762738\n",
            "Early stopping, best iteration is:\n",
            "[64]\ttraining's rmse: 0.0714134\tvalid_1's rmse: 0.0757401\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000521 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.392202\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0700987\tvalid_1's rmse: 0.0852743\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0767558\tvalid_1's rmse: 0.0848139\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000496 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.388708\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0695065\tvalid_1's rmse: 0.0894644\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.0733129\tvalid_1's rmse: 0.089243\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000477 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.390323\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0701505\tvalid_1's rmse: 0.0859927\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's rmse: 0.0751099\tvalid_1's rmse: 0.0858022\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000472 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.391642\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0699183\tvalid_1's rmse: 0.0865304\n",
            "Early stopping, best iteration is:\n",
            "[45]\ttraining's rmse: 0.0757818\tvalid_1's rmse: 0.0863732\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000633 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.392979\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0704253\tvalid_1's rmse: 0.0820317\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.0747972\tvalid_1's rmse: 0.0817401\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.395094\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0694835\tvalid_1's rmse: 0.0853483\n",
            "Early stopping, best iteration is:\n",
            "[78]\ttraining's rmse: 0.0714753\tvalid_1's rmse: 0.084642\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000487 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.391725\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0693699\tvalid_1's rmse: 0.0882929\n",
            "Early stopping, best iteration is:\n",
            "[67]\ttraining's rmse: 0.072103\tvalid_1's rmse: 0.0880538\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000492 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.393559\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0698378\tvalid_1's rmse: 0.0855907\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's rmse: 0.0751847\tvalid_1's rmse: 0.0854578\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.394680\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.069261\tvalid_1's rmse: 0.0870673\n",
            "Early stopping, best iteration is:\n",
            "[89]\ttraining's rmse: 0.0702673\tvalid_1's rmse: 0.0867113\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.395973\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0700313\tvalid_1's rmse: 0.0814029\n",
            "Early stopping, best iteration is:\n",
            "[53]\ttraining's rmse: 0.074995\tvalid_1's rmse: 0.0810392\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000483 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.394824\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0663765\tvalid_1's rmse: 0.0805241\n",
            "Early stopping, best iteration is:\n",
            "[77]\ttraining's rmse: 0.0683428\tvalid_1's rmse: 0.080335\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000463 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.391452\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0644002\tvalid_1's rmse: 0.0860997\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0715567\tvalid_1's rmse: 0.0853195\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000504 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.392912\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0671037\tvalid_1's rmse: 0.0810681\n",
            "Early stopping, best iteration is:\n",
            "[58]\ttraining's rmse: 0.0706262\tvalid_1's rmse: 0.0805457\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000462 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.394786\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0666743\tvalid_1's rmse: 0.0802684\n",
            "Early stopping, best iteration is:\n",
            "[96]\ttraining's rmse: 0.0669794\tvalid_1's rmse: 0.0802338\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000494 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.395904\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0673127\tvalid_1's rmse: 0.0776774\n",
            "Early stopping, best iteration is:\n",
            "[88]\ttraining's rmse: 0.068215\tvalid_1's rmse: 0.0775919\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000493 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.367071\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0647747\tvalid_1's rmse: 0.079996\n",
            "Early stopping, best iteration is:\n",
            "[60]\ttraining's rmse: 0.068524\tvalid_1's rmse: 0.0795224\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000460 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.363840\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0641505\tvalid_1's rmse: 0.0810322\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's rmse: 0.0711788\tvalid_1's rmse: 0.0804078\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000458 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.364452\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0653488\tvalid_1's rmse: 0.0786696\n",
            "Early stopping, best iteration is:\n",
            "[61]\ttraining's rmse: 0.0685222\tvalid_1's rmse: 0.078478\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.367244\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.065495\tvalid_1's rmse: 0.0801516\n",
            "[200]\ttraining's rmse: 0.0595539\tvalid_1's rmse: 0.080827\n",
            "Early stopping, best iteration is:\n",
            "[110]\ttraining's rmse: 0.0645743\tvalid_1's rmse: 0.0800763\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.367610\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0654182\tvalid_1's rmse: 0.0775592\n",
            "Early stopping, best iteration is:\n",
            "[52]\ttraining's rmse: 0.0701342\tvalid_1's rmse: 0.0769514\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000504 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.383766\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0649848\tvalid_1's rmse: 0.0815884\n",
            "Early stopping, best iteration is:\n",
            "[45]\ttraining's rmse: 0.0704149\tvalid_1's rmse: 0.0809265\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000470 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.381010\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0643071\tvalid_1's rmse: 0.0813382\n",
            "Early stopping, best iteration is:\n",
            "[57]\ttraining's rmse: 0.0683091\tvalid_1's rmse: 0.0807028\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000491 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.382245\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0658635\tvalid_1's rmse: 0.0790311\n",
            "Early stopping, best iteration is:\n",
            "[70]\ttraining's rmse: 0.0684195\tvalid_1's rmse: 0.0787454\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000449 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384088\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0652578\tvalid_1's rmse: 0.0813185\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's rmse: 0.0710243\tvalid_1's rmse: 0.0812132\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000482 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.384949\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0664694\tvalid_1's rmse: 0.0754201\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0709528\tvalid_1's rmse: 0.0751022\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000475 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.360134\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0610057\tvalid_1's rmse: 0.0758715\n",
            "Early stopping, best iteration is:\n",
            "[90]\ttraining's rmse: 0.0618559\tvalid_1's rmse: 0.0757785\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000521 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.357548\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0598059\tvalid_1's rmse: 0.0802227\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's rmse: 0.0634126\tvalid_1's rmse: 0.0798146\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000465 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.359044\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.061375\tvalid_1's rmse: 0.0751686\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0666836\tvalid_1's rmse: 0.0749416\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000485 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.361234\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0611474\tvalid_1's rmse: 0.0760866\n",
            "[200]\ttraining's rmse: 0.0552778\tvalid_1's rmse: 0.0766612\n",
            "Early stopping, best iteration is:\n",
            "[101]\ttraining's rmse: 0.0610758\tvalid_1's rmse: 0.0760697\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000497 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.360789\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.062195\tvalid_1's rmse: 0.0713848\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's rmse: 0.0675203\tvalid_1's rmse: 0.0710953\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000528 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.321530\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0546855\tvalid_1's rmse: 0.0680231\n",
            "Early stopping, best iteration is:\n",
            "[70]\ttraining's rmse: 0.0569892\tvalid_1's rmse: 0.0677437\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000467 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.319644\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0540512\tvalid_1's rmse: 0.0700555\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's rmse: 0.0594122\tvalid_1's rmse: 0.0691689\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5414, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000478 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.320374\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0542255\tvalid_1's rmse: 0.0691119\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's rmse: 0.0589918\tvalid_1's rmse: 0.0683801\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000454 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.322729\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0555768\tvalid_1's rmse: 0.0687511\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's rmse: 0.0574661\tvalid_1's rmse: 0.0685188\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 317\n",
            "[LightGBM] [Info] Number of data points in the train set: 5415, number of used features: 6\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 6 dense feature groups (0.04 MB) transferred to GPU in 0.000449 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 0.322847\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's rmse: 0.0556537\tvalid_1's rmse: 0.0645993\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's rmse: 0.059207\tvalid_1's rmse: 0.0643576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "test3[\"env2\"] = test3[\"env\"] / test3[\"observed_max\"]\n",
        "test3[\"observed\"] = test3[\"nv2\"]*test3[\"observed_max\"]\n",
        "test3[\"fluctuation\"] = np.abs((test3[\"observed\"] - test3[\"preal-30\"])/test3[\"observed_max\"])\n",
        "test3_result = pd.DataFrame(columns=[\"id\",\"rmse\",\"mape\",\"mae_per\",\"rmse_op\",\"mape_op\",\"nrmse\",\"nmae\",\"r2\",\"mae_per_op\",\"nrmse_op\",\"nmae_op\",\"r2_op\",\"fluctuation\",\"observed_max\"])\n",
        "for id,group in test3.groupby(\"id\"):\n",
        "  mape = (np.abs((group.loc[group.nv2!=0,\"pred\"] - group.loc[group.nv2!=0,\"nv2\"])/group.loc[group.nv2!=0,\"nv2\"])).mean()\n",
        "  rmse = np.sqrt(mean_squared_error(group[\"nv2\"], group[\"pred\"]))\n",
        "  mae_per = np.abs(group[\"pred\"] - group[\"nv2\"]).mean()\n",
        "  nrmse = np.sqrt(np.mean(((group.pred-group.nv2)/group.nv2.max())**(2))) \n",
        "  nmae = np.abs(group.pred-group.nv2).sum()/group.nv2.sum() \n",
        "  r2 = r2_score(group[\"nv2\"], group[\"pred\"])\n",
        "\n",
        "  mape_op = (np.abs((group.loc[group.nv2!=0,\"env2\"] - group.loc[group.nv2!=0,\"nv2\"])/group.loc[group.nv2!=0,\"nv2\"])).mean()\n",
        "  rmse_op = np.sqrt(mean_squared_error(group[\"nv2\"], group[\"env2\"]))\n",
        "  mae_per_op = np.abs(group[\"pred\"] - group[\"env2\"]).mean()\n",
        "  nrmse_op = np.sqrt(np.mean(((group.env2-group.nv2)/group.nv2.max())**(2)))\n",
        "  nmae_op = np.abs(group.env2-group.nv2).sum()/group.nv2.sum()\n",
        "  r2_op = r2_score(group[\"env2\"], group[\"pred\"])\n",
        "\n",
        "  fluctuation = group[\"fluctuation\"].mean()\n",
        "  observed_max = group[\"observed_max\"].iloc[0]\n",
        "\n",
        "  test3_result = test3_result.append({\"id\":id,\"rmse\":rmse,\"mape\":mape,\"mae_per\":mae_per,\"nrmse\":nrmse,\"nmae\":nmae,\"r2\":r2,\"rmse_op\":rmse_op,\"mape_op\":mape_op,\"mae_per_op\":mae_per_op,\"nrmse_op\":nrmse_op,\\\n",
        "                                    \"nmae_op\":nmae_op,\"r2_op\":r2_op,\"fluctuation\":fluctuation,\"observed_max\":observed_max,\"observed_max_test\":group.nv2.max()},ignore_index=True)\n",
        "\n",
        "df_test3 = pd.merge(df_missing,test3_result,on=\"id\")\n",
        "df_test3 = df_test3[df_test3.missing_percent_test==0]"
      ],
      "metadata": {
        "id": "j_GYm3HFlEtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_ = pd.DataFrame()\n",
        "scores_= scores_.append({\"month\":month_list,\n",
        "                         #\"train.shape[0]\":train.shape[0],\n",
        "                        #  \"train_mape\":(np.abs((train.loc[train.nv2!=0,\"pred\"] - train.loc[train.nv2!=0,\"nv2\"])/train.loc[train.nv2!=0,\"nv2\"])).mean(),\n",
        "                        # \"valid_mape\":(np.abs((valid.loc[valid.nv2!=0,\"pred\"] - valid.loc[valid.nv2!=0,\"nv2\"])/valid.loc[valid.nv2!=0,\"nv2\"])).mean(),\n",
        "                          \"test_mape\":(np.abs((test3.loc[test3.nv2!=0,\"pred\"] - test3.loc[test3.nv2!=0,\"nv2\"])/test3.loc[test3.nv2!=0,\"nv2\"])).mean(),\n",
        "                          #\"train_rmse\":np.sqrt(mean_squared_error(train[\"nv2\"], train[\"pred\"])),\n",
        "                        # \"valid_rmse\":np.sqrt(mean_squared_error(valid[\"nv2\"], valid[\"pred\"])),\n",
        "                          \"test_rmse\":np.sqrt(mean_squared_error(test3[\"nv2\"], test3[\"pred\"]))}\n",
        "                        ,ignore_index=True)"
      ],
      "metadata": {
        "id": "GojPNlf_ix6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "f8sbUw0-jZf3",
        "outputId": "19567a43-6e3d-41eb-ff10-f364d5cf77b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  month  test_mape  test_rmse\n",
              "0  [6, 5, 4, 3, 2, 1, 12, 11, 10, 9, 8]   0.258194   0.097654"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19a18e69-1165-40e3-bd60-6a5de7df6ad8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>test_mape</th>\n",
              "      <th>test_rmse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[6, 5, 4, 3, 2, 1, 12, 11, 10, 9, 8]</td>\n",
              "      <td>0.258194</td>\n",
              "      <td>0.097654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19a18e69-1165-40e3-bd60-6a5de7df6ad8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-19a18e69-1165-40e3-bd60-6a5de7df6ad8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-19a18e69-1165-40e3-bd60-6a5de7df6ad8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test3.to_csv(\"./output/single/test3.csv\",index=False)\n",
        "df_test3.to_csv(\"./output/single/df_test3.csv\",index=False)"
      ],
      "metadata": {
        "id": "QtMs7aCtjcQ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}